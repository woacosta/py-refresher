{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "## Lambda expressions review\n\nOne of Pythons most useful (and for beginners, confusing) tools is the lambda expression. lambda expressions allow us to create \"anonymous\" functions. This basically means we can quickly make ad-hoc functions without needing to properly define a function using def.\n\nFunction objects returned by running lambda expressions work exactly the same as those created and assigned by defs. There is key difference that makes lambda useful in specialized roles:\n\n**lambda's body is a single expression, not a block of statements.**\n\n* The lambda's body is similar to what we would put in a def body's return statement. We simply type the result as an expression instead of explicitly returning it. Because it is limited to an expression, a lambda is less general that a def. We can only squeeze design, to limit program nesting. lambda is designed for coding simple functions, and def handles the larger tasks.\n\nLets slowly break down a lambda expression by deconstructing a function:"}, {"cell_type": "markdown", "metadata": {}, "source": "A regular function definition"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "def square(num):\n    result = num**2\n    return result"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"data": {"text/plain": "16"}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": "square(4)"}, {"cell_type": "markdown", "metadata": {}, "source": "Bad style but you can still make the above into one line"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"data": {"text/plain": "16"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "def square(num):return num**2\nsquare(4)"}, {"cell_type": "markdown", "metadata": {}, "source": "use a lambda expression to write the same thing"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"data": {"text/plain": "<function __main__.<lambda>(num)>"}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": "lambda num: num**2"}, {"cell_type": "markdown", "metadata": {}, "source": "usually wouldn't assign lambda expressions but we're doing it here to show that they work"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"data": {"text/plain": "9"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "sq = lambda num: num**2\nsq(3)"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"data": {"text/plain": "False"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": "even = lambda num: num%2 == 0\neven(9)"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"data": {"text/plain": "'a'"}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": "first = lambda s: s[0]\nfirst('asdfasdf')"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"data": {"text/plain": "'fdsafdsa'"}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": "rev = lambda s: s[::-1]\nrev('asdfasdf')"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"data": {"text/plain": "7"}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": "adder = lambda x,y: x+y\nadder(3,4)"}, {"cell_type": "markdown", "metadata": {}, "source": "lambda expressions really shine when used in conjunction with map(),filter() and reduce(). Each of those functions has its own lecture, so feel free to explore them if your very itnerested in lambda."}, {"cell_type": "markdown", "metadata": {}, "source": "# Introduction to Spark and Python\n\n\n## Creating a SparkContext\n\nFirst we need to create a SparkContext. We will import this from pyspark:"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": "from pyspark import SparkContext"}, {"cell_type": "markdown", "metadata": {}, "source": "Now create the SparkContext,A SparkContext represents the connection to a Spark cluster, and can be used to create an RDD and broadcast variables on that cluster.\n\n*Note! You can only have one SparkContext at a time the way we are running things here.*"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": "sc = SparkContext()"}, {"cell_type": "markdown", "metadata": {}, "source": "Let's write an example text file to read, we'll use some special jupyter notebook commands for this. The magic command %% makes anything written into a cell into a file. IF USING A CLUSTER, MAYBE JUST TRY LOADING THE FILE FROM gs://"}, {"cell_type": "code", "execution_count": 71, "metadata": {"scrolled": true}, "outputs": [], "source": "#%%writefile example.txt\n#first line\n#second line\n#third line\n#fourth line"}, {"cell_type": "markdown", "metadata": {}, "source": "## Creating the RDD\n\nNow we can take in the textfile using the textFile method off of the SparkContext we created. This method will read a text file from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI, and return it as an RDD of Strings"}, {"cell_type": "code", "execution_count": 63, "metadata": {}, "outputs": [], "source": "textFile = sc.textFile('gs://wac-buck/notebooks/jupyter/21-Big-Data-and-Spark/example.txt')"}, {"cell_type": "markdown", "metadata": {}, "source": "Spark\u2019s primary abstraction is a distributed collection of items called a Resilient Distributed Dataset (RDD). RDDs can be created from Hadoop InputFormats (such as HDFS files) or by transforming other RDDs. \n\n### Actions\n\nWe have just created an RDD using the textFile method and can perform operations on this object, such as counting the rows.\n\nRDDs have actions, which return values, and transformations, which return pointers to new RDDs. Let\u2019s start with a few actions:"}, {"cell_type": "code", "execution_count": 64, "metadata": {}, "outputs": [{"data": {"text/plain": "4"}, "execution_count": 64, "metadata": {}, "output_type": "execute_result"}], "source": "textFile.count()"}, {"cell_type": "markdown", "metadata": {}, "source": "Note that if you get an error you might be looking for the file in the wrong place. Consider whether the file is stored locally or on a cloud etc"}, {"cell_type": "code", "execution_count": 65, "metadata": {}, "outputs": [{"data": {"text/plain": "'first line'"}, "execution_count": 65, "metadata": {}, "output_type": "execute_result"}], "source": "textFile.first()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Transformations\n\nNow we can use transformations, for example the filter transformation will return a new RDD with a subset of items in the file. Let's create a sample transformation using the filter() method. This method (just like Python's own filter function) will only return elements that satisfy the condition. Let's try looking for lines that contain the word 'second'. In which case, there should only be one line that has that."}, {"cell_type": "code", "execution_count": 66, "metadata": {}, "outputs": [], "source": "# for each line, return the line where the string 'second' is True\nsecfind = textFile.filter(lambda line: 'second' in line)"}, {"cell_type": "code", "execution_count": 67, "metadata": {}, "outputs": [{"data": {"text/plain": "PythonRDD[36] at RDD at PythonRDD.scala:53"}, "execution_count": 67, "metadata": {}, "output_type": "execute_result"}], "source": "# RDD\nsecfind"}, {"cell_type": "code", "execution_count": 68, "metadata": {}, "outputs": [{"data": {"text/plain": "['second line']"}, "execution_count": 68, "metadata": {}, "output_type": "execute_result"}], "source": "# Perform action on transformation\nsecfind.collect()"}, {"cell_type": "code", "execution_count": 69, "metadata": {}, "outputs": [{"data": {"text/plain": "1"}, "execution_count": 69, "metadata": {}, "output_type": "execute_result"}], "source": "# Perform action on transformation\nsecfind.count()"}, {"cell_type": "markdown", "metadata": {}, "source": "Notice how the transformations won't display an output and won't be run until an action is called. In the next lecture: Advanced Spark and Python we will begin to see many more examples of this transformation and action relationship!"}, {"cell_type": "markdown", "metadata": {}, "source": "# RDD Transformations and Actions\n\nIn this lecture we will begin to delve deeper into using Spark and Python. Please view the video lecture for a full explanation.\n\n## Important Terms\n\nLet's quickly go over some important terms:\n\nTerm                   |Definition\n----                   |-------\nRDD                    |Resilient Distributed Dataset\nTransformation         |Spark operation that produces an RDD\nAction                 |Spark operation that produces a local object\nSpark Job              |Sequence of transformations on data with a final action"}, {"cell_type": "markdown", "metadata": {}, "source": "## Creating an RDD\n\nThere are two common ways to create an RDD:\n\nMethod                      |Result\n----------                               |-------\n`sc.parallelize(array)`                  |Create RDD of elements of array (or list)\n`sc.textFile(path/to/file)`                      |Create RDD of lines from file"}, {"cell_type": "markdown", "metadata": {}, "source": "## RDD Transformations\n\nWe can use transformations to create a set of instructions we want to preform on the RDD (before we call an action and actually execute them).\n\nTransformation Example                          |Result\n----------                               |-------\n`filter(lambda x: x % 2 == 0)`           |Discard non-even elements\n`map(lambda x: x * 2)`                   |Multiply each RDD element by `2`\n`map(lambda x: x.split())`               |Split each string into words\n`flatMap(lambda x: x.split())`           |Split each string into words and flatten sequence\n`sample(withReplacement=True,0.25)`      |Create sample of 25% of elements with replacement\n`union(rdd)`                             |Append `rdd` to existing RDD\n`distinct()`                             |Remove duplicates in RDD\n`sortBy(lambda x: x, ascending=False)`   |Sort elements in descending order"}, {"cell_type": "markdown", "metadata": {}, "source": "## RDD Actions\n\nOnce you have your 'recipe' of transformations ready, what you will do next is execute them by calling an action. Here are some common actions:\n\nAction                             |Result\n----------                             |-------\n`collect()`                            |Convert RDD to in-memory list \n`take(3)`                              |First 3 elements of RDD \n`top(3)`                               |Top 3 elements of RDD\n`takeSample(withReplacement=True,3)`   |Create sample of 3 elements with replacement\n`sum()`                                |Find element sum (assumes numeric elements)\n`mean()`                               |Find element mean (assumes numeric elements)\n`stdev()`                              |Find element deviation (assumes numeric elements)"}, {"cell_type": "markdown", "metadata": {}, "source": "# Examples\n\nNow the best way to show all of this is by going through examples! We'll first review a bit by creating and working with a simple text file, then we will move on to more realistic data, such as customers and sales data.\n\n### Creating an RDD from a text file:\n\n** Creating the textfile **"}, {"cell_type": "code", "execution_count": 76, "metadata": {}, "outputs": [], "source": "text_rdd = sc.textFile('gs://wac-buck/notebooks/jupyter/21-Big-Data-and-Spark/example2.txt')"}, {"cell_type": "code", "execution_count": 77, "metadata": {}, "outputs": [{"data": {"text/plain": "['first ', 'second line', 'the third line', 'then a fourth line']"}, "execution_count": 77, "metadata": {}, "output_type": "execute_result"}], "source": "text_rdd.collect()"}, {"cell_type": "code", "execution_count": 73, "metadata": {}, "outputs": [{"data": {"text/plain": "[['first'],\n ['second', 'line'],\n ['the', 'third', 'line'],\n ['then', 'a', 'fourth', 'line']]"}, "execution_count": 73, "metadata": {}, "output_type": "execute_result"}], "source": "# Map a function (or lambda expression) to each line\n# Then collect the results.\ntext_rdd.map(lambda line: line.split()).collect()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Map vs flatMap"}, {"cell_type": "code", "execution_count": 74, "metadata": {}, "outputs": [{"data": {"text/plain": "['first',\n 'second',\n 'line',\n 'the',\n 'third',\n 'line',\n 'then',\n 'a',\n 'fourth',\n 'line']"}, "execution_count": 74, "metadata": {}, "output_type": "execute_result"}], "source": "# Collect everything as a single flat map\ntext_rdd.flatMap(lambda line: line.split()).collect()"}, {"cell_type": "markdown", "metadata": {}, "source": "# RDDs and Key Value Pairs\n\nNow that we've worked with RDDs and how to aggregate values with them, we can begin to look into working with Key Value Pairs. In order to do this, let's create some fake data as a new text file.\n\nThis data represents some services sold to customers for some SAAS business."}, {"cell_type": "code", "execution_count": 78, "metadata": {}, "outputs": [], "source": "services = sc.textFile('gs://wac-buck/notebooks/jupyter/21-Big-Data-and-Spark/services.txt')"}, {"cell_type": "code", "execution_count": 79, "metadata": {}, "outputs": [{"data": {"text/plain": "['#EventId    Timestamp    Customer   State    ServiceID    Amount',\n '201       10/13/2017      100       NY       131          100.00',\n '204       10/18/2017      700       TX       129          450.00',\n '202       10/15/2017      203       CA       121          200.00']"}, "execution_count": 79, "metadata": {}, "output_type": "execute_result"}], "source": "services.take(4)"}, {"cell_type": "code", "execution_count": 80, "metadata": {}, "outputs": [{"data": {"text/plain": "PythonRDD[47] at RDD at PythonRDD.scala:53"}, "execution_count": 80, "metadata": {}, "output_type": "execute_result"}], "source": "services.map(lambda x: x.split())"}, {"cell_type": "code", "execution_count": 81, "metadata": {}, "outputs": [{"data": {"text/plain": "[['#EventId', 'Timestamp', 'Customer', 'State', 'ServiceID', 'Amount'],\n ['201', '10/13/2017', '100', 'NY', '131', '100.00'],\n ['204', '10/18/2017', '700', 'TX', '129', '450.00']]"}, "execution_count": 81, "metadata": {}, "output_type": "execute_result"}], "source": "services.map(lambda x: x.split()).take(3)"}, {"cell_type": "code", "execution_count": 82, "metadata": {}, "outputs": [{"data": {"text/plain": "['EventId    Timestamp    Customer   State    ServiceID    Amount',\n '201       10/13/2017      100       NY       131          100.00',\n '204       10/18/2017      700       TX       129          450.00',\n '202       10/15/2017      203       CA       121          200.00',\n '206       10/19/2017      202       CA       131          500.00',\n '203       10/17/2017      101       NY       173          750.00',\n '205       10/19/2017      202       TX       121          200.00']"}, "execution_count": 82, "metadata": {}, "output_type": "execute_result"}], "source": "# Remove the hash in eventID\n# for each x, return elements from position 1 to the end if\n# the first element (0) isn't a hash. else just return the whole value\nservices.map(lambda x: x[1:] if x[0]=='#' else x).collect()"}, {"cell_type": "code", "execution_count": 83, "metadata": {}, "outputs": [{"data": {"text/plain": "[['EventId', 'Timestamp', 'Customer', 'State', 'ServiceID', 'Amount'],\n ['201', '10/13/2017', '100', 'NY', '131', '100.00'],\n ['204', '10/18/2017', '700', 'TX', '129', '450.00'],\n ['202', '10/15/2017', '203', 'CA', '121', '200.00'],\n ['206', '10/19/2017', '202', 'CA', '131', '500.00'],\n ['203', '10/17/2017', '101', 'NY', '173', '750.00'],\n ['205', '10/19/2017', '202', 'TX', '121', '200.00']]"}, "execution_count": 83, "metadata": {}, "output_type": "execute_result"}], "source": "# same command but now add a map to split all x into strings\n# after the hash operation has finished\nservices.map(lambda x: x[1:] if x[0]=='#' else x).map(lambda x: x.split()).collect()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Using Key Value Pairs for Operations\n\nLet us now begin to use methods that combine lambda expressions that use a ByKey argument. These ByKey methods will assume that your data is in a Key,Value form. \n\n\nFor example let's find out the total sales per state:"}, {"cell_type": "code", "execution_count": 85, "metadata": {}, "outputs": [], "source": "# From Previous\ncleanServ = services.map(lambda x: x[1:] if x[0]=='#' else x).map(lambda x: x.split())"}, {"cell_type": "code", "execution_count": 86, "metadata": {}, "outputs": [{"data": {"text/plain": "[['EventId', 'Timestamp', 'Customer', 'State', 'ServiceID', 'Amount'],\n ['201', '10/13/2017', '100', 'NY', '131', '100.00'],\n ['204', '10/18/2017', '700', 'TX', '129', '450.00'],\n ['202', '10/15/2017', '203', 'CA', '121', '200.00'],\n ['206', '10/19/2017', '202', 'CA', '131', '500.00'],\n ['203', '10/17/2017', '101', 'NY', '173', '750.00'],\n ['205', '10/19/2017', '202', 'TX', '121', '200.00']]"}, "execution_count": 86, "metadata": {}, "output_type": "execute_result"}], "source": "cleanServ.collect()"}, {"cell_type": "code", "execution_count": 87, "metadata": {}, "outputs": [{"data": {"text/plain": "[('State', 'Amount'),\n ('NY', '100.00'),\n ('TX', '450.00'),\n ('CA', '200.00'),\n ('CA', '500.00'),\n ('NY', '750.00'),\n ('TX', '200.00')]"}, "execution_count": 87, "metadata": {}, "output_type": "execute_result"}], "source": "# start by grabbing some fields\ncleanServ.map(lambda lst: (lst[3],lst[-1])).collect()"}, {"cell_type": "code", "execution_count": 88, "metadata": {}, "outputs": [{"data": {"text/plain": "[('State', 'Amount'),\n ('NY', '100.00750.00'),\n ('TX', '450.00200.00'),\n ('CA', '200.00500.00')]"}, "execution_count": 88, "metadata": {}, "output_type": "execute_result"}], "source": "# Continue with reduceByKey\n# Notice how it assumes that the first item is the key!\ncleanServ.map(lambda lst: (lst[3],lst[-1]))\\\n        .reduceByKey(lambda amt1,amt2: amt1+amt2)\\\n        .collect()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Note that the number is actually a string so you accidentally concatenated two strings. make the strings into floats to perform math on them"}, {"cell_type": "code", "execution_count": 90, "metadata": {}, "outputs": [{"data": {"text/plain": "[('State', 'Amount'), ('NY', 850.0), ('TX', 650.0), ('CA', 700.0)]"}, "execution_count": 90, "metadata": {}, "output_type": "execute_result"}], "source": "cleanServ.map(lambda lst: (lst[3],lst[-1]))\\\n        .reduceByKey(lambda amt1,amt2: float(amt1)+float(amt2))\\\n        .collect()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Great! Now the states have been grouped and summed. Remember that to use .reduceByKey you have to have your data in pair form."}, {"cell_type": "markdown", "metadata": {}, "source": "We can continue our analysis by sorting this output:"}, {"cell_type": "code", "execution_count": 91, "metadata": {}, "outputs": [{"data": {"text/plain": "[('NY', 850.0), ('TX', 650.0), ('CA', 700.0)]"}, "execution_count": 91, "metadata": {}, "output_type": "execute_result"}], "source": "# Grab state and amounts\n# Add them\n# Get rid of ('State','Amount')\ncleanServ.map(lambda lst: (lst[3],lst[-1]))\\\n.reduceByKey(lambda amt1,amt2 : float(amt1)+float(amt2))\\\n.filter(lambda x: not x[0]=='State')\\\n.collect()"}, {"cell_type": "code", "execution_count": 92, "metadata": {}, "outputs": [{"data": {"text/plain": "[('NY', 850.0), ('CA', 700.0), ('TX', 650.0)]"}, "execution_count": 92, "metadata": {}, "output_type": "execute_result"}], "source": "# Grab state and amounts\n# Add them\n# Get rid of ('State','Amount')\n# Sort them by the amount value\ncleanServ.map(lambda lst: (lst[3],lst[-1]))\\\n.reduceByKey(lambda amt1,amt2 : float(amt1)+float(amt2))\\\n.filter(lambda x: not x[0]=='State')\\\n.sortBy(lambda stateAmount: stateAmount[1], ascending=False)\\\n.collect()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Remember to try to use unpacking for readability. For example see below. \nThe second function is much more readable when you look at it later. You can easily see that you want to grab the amount. The first function only tells you that you want the last item."}, {"cell_type": "code", "execution_count": 93, "metadata": {}, "outputs": [], "source": "x = ['ID','State','Amount']"}, {"cell_type": "code", "execution_count": 94, "metadata": {}, "outputs": [], "source": "def func1(lst):\n    return lst[-1]"}, {"cell_type": "code", "execution_count": 95, "metadata": {}, "outputs": [{"data": {"text/plain": "'Amount'"}, "execution_count": 95, "metadata": {}, "output_type": "execute_result"}], "source": "func1(x)"}, {"cell_type": "code", "execution_count": 96, "metadata": {}, "outputs": [], "source": "def func2(id_st_amt):\n    # Unpack Values\n    (Id,st,amt) = id_st_amt\n    return amt"}, {"cell_type": "code", "execution_count": 97, "metadata": {}, "outputs": [{"data": {"text/plain": "'Amount'"}, "execution_count": 97, "metadata": {}, "output_type": "execute_result"}], "source": "func2(x)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.10"}}, "nbformat": 4, "nbformat_minor": 2}